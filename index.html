<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <title>リアルタイム感情認識カメラ</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
  <style>
    body { font-family: 'Arial', sans-serif; margin: 0; background: #2c3e50; color: #ecf0f1; display: flex; flex-direction: column; align-items: center; justify-content: flex-start; min-height: 100vh; overflow: hidden; padding-bottom: 20px; }
    #info-text { font-size: 32px; font-weight: bold; padding: 20px 10px 10px; text-align: center; width: 100%; box-sizing: border-box; color: #f39c12; }
    #emoji-display { font-size: 90px; text-align: center; width: 100%; padding: 0 10px 20px; line-height: 1; }
    #video-container { position: relative; width: 95vw; max-width: 600px; margin: 0 auto; border-radius: 15px; overflow: hidden; box-shadow: 0 8px 16px rgba(0, 0, 0, 0.4); background-color: #34495e; }
    #video-container::before { content: ''; display: block; padding-top: calc(3 / 4 * 100%); }
    video, canvas { position: absolute; top: 0; left: 0; width: 100%; height: 100%; object-fit: cover; transform: scaleX(-1); }
    canvas { z-index: 10; }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js" crossorigin="anonymous"></script>
</head>

<body>
  <div id="info-text">AIモデルを準備中...</div>
  <div id="emoji-display">⏳</div>
  <div id="video-container">
    <video id="input_video" autoplay muted playsinline></video>
    <canvas id="output_canvas"></canvas>
  </div>

  <script>
    const videoElement = document.getElementById('input_video');
    const canvasElement = document.getElementById('output_canvas');
    const canvasCtx = canvasElement.getContext('2d');
    const infoText = document.getElementById('info-text');
    const emojiDisplay = document.getElementById('emoji-display');

    const emotionEmojis = {
      "Neutral": "😐", "Happy": "😊", "Sad": "😔", "Angry": "😠", "Surprise": "😮", "No Face": "🤔"
    };

    function onResults(results) {
      if (!results.image || videoElement.videoWidth === 0) { return; }
      canvasElement.width = videoElement.videoWidth;
      canvasElement.height = videoElement.videoHeight;
      canvasCtx.save();
      canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
      canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);
      let emotion = "No Face";
      if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
        emotion = "Neutral";
        const landmarks = results.multiFaceLandmarks[0];
        drawConnectors(canvasCtx, landmarks, FACEMESH_TESSELATION, {color: '#80DEEA', lineWidth: 1});

        // --- 感情判定ロジック (最終チューニング版) ---
        const SURPRISE_THRESHOLD = 0.38; // Surpriseの感度をわずかに上げる
        const HAPPY_THRESHOLD = 0.02;    // Happyの感度をさらに上げる
        const SAD_THRESHOLD = 0.035;

        // Angryの新しい基準と調整
        const ANGRY_LOWER_THRESHOLD = 0.1; // 眉の下がり具合の基準
        const ANGRY_FURROW_THRESHOLD = 0.23; // 眉間の寄り具合の基準
        // 新しいAngry判定: 眉頭から鼻先への距離の短縮
        const ANGRY_NOSE_BROW_DISTANCE_THRESHOLD = 0.45; // 眉頭と鼻先の距離がこの値以下だとAngry候補

        const interEyeDistance = Math.hypot(landmarks[263].x - landmarks[33].x, landmarks[263].y - landmarks[33].y);
        const mouthLeft = landmarks[61], mouthRight = landmarks[291], mouthTop = landmarks[13], mouthBottom = landmarks[14];
        const eyebrowLeftInner = landmarks[293], eyebrowRightInner = landmarks[63];
        const eyeLeftInner = landmarks[263], eyeRightInner = landmarks[33];
        const noseBottom = landmarks[2];
        const noseTip = landmarks[1]; // 鼻の先端

        // Surprise
        const mouthHeight = Math.hypot(mouthTop.x - mouthBottom.x, mouthTop.y - mouthBottom.y);
        const normalizedMouthHeight = mouthHeight / interEyeDistance;
        const isSurprised = normalizedMouthHeight > SURPRISE_THRESHOLD;
        
        // Happy
        const mouthCornerY = (mouthLeft.y + mouthRight.y) / 2;
        const smileIntensity = noseBottom.y - mouthCornerY;
        const normalizedSmileIntensity = smileIntensity / interEyeDistance;
        const isHappy = normalizedSmileIntensity > HAPPY_THRESHOLD;

        // Sad
        const mouthCenterY = (mouthTop.y + mouthBottom.y) / 2;
        const mouthCornerDrop = mouthCornerY - mouthCenterY;
        const normalizedMouthCornerDrop = mouthCornerDrop / interEyeDistance;
        const isSad = normalizedMouthCornerDrop > SAD_THRESHOLD;

        // Angry (組み合わせ条件)
        const eyebrowLowerAmount = ((eyebrowLeftInner.y + eyebrowRightInner.y) / 2) - ((eyeLeftInner.y + eyeRightInner.y) / 2);
        const normalizedEyebrowLower = eyebrowLowerAmount / interEyeDistance;
        const isBrowLowered = normalizedEyebrowLower < ANGRY_LOWER_THRESHOLD; // 眉が十分に下がっているか

        const innerEyebrowDistance = Math.hypot(eyebrowLeftInner.x - eyebrowRightInner.x, eyebrowLeftInner.y - eyebrowRightInner.y);
        const normalizedInnerEyebrowDistance = innerEyebrowDistance / interEyeDistance;
        const isBrowFurrowed = normalizedInnerEyebrowDistance < ANGRY_FURROW_THRESHOLD; // 眉間が十分に寄っているか
        
        // 新しいAngry条件: 眉頭と鼻先の距離が短くなっているか
        const browNoseDistance = Math.hypot(eyebrowLeftInner.x - noseTip.x, eyebrowLeftInner.y - noseTip.y); // 左眉頭と鼻先
        const normalizedBrowNoseDistance = browNoseDistance / interEyeDistance;
        const isFaceCentralizedAngry = normalizedBrowNoseDistance < ANGRY_NOSE_BROW_DISTANCE_THRESHOLD; // 眉頭と鼻先が近い

        // Angry判定: 眉が下がり、かつ(眉間が寄る または 顔が中心に集まる)
        const isAngry = isBrowLowered && (isBrowFurrowed || isFaceCentralizedAngry);


        // --- 条件に基づいて最終的な感情を決定 (優先度順) ---
        if (isSurprised) { emotion = "Surprise"; } 
        else if (isAngry) { emotion = "Angry"; } // AngryはHappyより先に判定
        else if (isHappy) { emotion = "Happy"; }
        else if (isSad) { emotion = "Sad"; }
        // どれにも当てはまらない場合はNeutralがそのままemotionになる
      }
      infoText.innerText = (emotion === "No Face") ? "顔を検出中... 🤔" : `Emotion: ${emotion}`;
      emojiDisplay.innerText = emotionEmojis[emotion];
      canvasCtx.restore();
    }

    const faceMesh = new FaceMesh({locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`});
    faceMesh.setOptions({ maxNumFaces: 1, refineLandmarks: true, minDetectionConfidence: 0.5, minTrackingConfidence: 0.5, modelComplexity: 0 });
    // AIモデルの読み込み完了を検知
    faceMesh.onLoaded = () => {
      infoText.innerText = "カメラにアクセス中...";
      emojiDisplay.innerText = "📷";
      // モデル読み込み完了後にカメラを起動
      camera.start().catch(e => {
          console.error("Camera start failed:", e);
          infoText.innerText = "エラー: カメラ起動に失敗。許可を確認してください。";
          emojiDisplay.innerText = "🚫";
      });
    };
    faceMesh.onResults(onResults);

    const camera = new Camera(videoElement, {
      onFrame: async () => { await faceMesh.send({image: videoElement}); },
      width: 640,
      height: 480
    });
    
    // ページロード時にモデルの読み込みを開始 (カメラはモデル読み込み後に起動)
    window.addEventListener('load', () => {
        // AIモデルの読み込みは自動的に行われる
        infoText.innerText = "AIモデルを準備中... ⏳";
        emojiDisplay.innerText = "⏳";
    });

    window.addEventListener('beforeunload', () => {
      camera.stop();
      if (videoElement.srcObject) { videoElement.srcObject.getTracks().forEach(track => track.stop()); }
    });
  </script>
</body>
</html>
