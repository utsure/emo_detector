<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <title>リアルタイム感情認識カメラ</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
  <style>
    body {
      font-family: 'Arial', sans-serif;
      margin: 0;
      background: #2c3e50; /* 濃い青系の背景 */
      color: #ecf0f1; /* 明るい文字色 */
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: flex-start; /* 上部に寄せる */
      min-height: 100vh; /* 画面いっぱいに広げる */
      overflow: hidden; /* スクロールバーが出ないように */
    }

    #info-text {
      font-size: 32px; /* 感情表示の文字を大きく */
      font-weight: bold;
      padding: 20px 10px;
      text-align: center;
      width: 100%;
      box-sizing: border-box;
      color: #f39c12; /* オレンジ系の強調色 */
    }

    #video-container {
      position: relative;
      width: 95vw; /* 画面幅の95%を使用 */
      max-width: 600px; /* 最大幅を設定してPCでも見やすく */
      margin: 20px auto; /* 上下の余白と中央寄せ */
      border-radius: 15px; /* 角を丸く */
      overflow: hidden; /* 子要素がはみ出さないように */
      box-shadow: 0 8px 16px rgba(0, 0, 0, 0.4); /* 影で立体感を出す */
      background-color: #34495e; /* コンテナの背景色 */
    }

    /* ビデオとキャンバスの親要素にアスペクト比を維持させるハック */
    #video-container::before {
      content: '';
      display: block;
      padding-top: calc(3 / 4 * 100%); /* 4:3のアスペクト比 (スマホカメラは4:3が多い) */
      /* もしカメラが16:9なら padding-top: calc(9 / 16 * 100%); に変更 */
    }

    video, canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      object-fit: cover; /* 映像をコンテナに合わせてトリミング */
      transform: scaleX(-1); /* 左右反転 (鏡像表示) */
    }

    /* MediaPipeの描画が前面にくるようにz-indexを設定 */
    canvas {
      z-index: 10;
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js" crossorigin="anonymous"></script>
</head>

<body>
  <div id="info-text">Emotion: Neutral</div>
  <div id="video-container">
    <video id="input_video" autoplay muted playsinline></video>
    <canvas id="output_canvas"></canvas>
  </div>

  <script>
    const videoElement = document.getElementById('input_video');
    const canvasElement = document.getElementById('output_canvas');
    const canvasCtx = canvasElement.getContext('2d');
    const infoText = document.getElementById('info-text');

    function onResults(results) {
      // 映像のサイズがまだ確定していない、またはMediaPipeの結果がない場合は処理を中断
      if (!results.image || videoElement.videoWidth === 0 || !results.multiFaceLandmarks || results.multiFaceLandmarks.length === 0) {
        // 顔が検出されない場合はNeutralを表示
        infoText.innerText = "Emotion: Neutral (顔を検出中...)";
        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height); // 顔がない場合はメッシュも消す
        canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height); // 映像だけは表示
        return;
      }
      
      // キャンバスのサイズをビデオの実際の解像度に合わせる
      canvasElement.width = videoElement.videoWidth;
      canvasElement.height = videoElement.videoHeight;

      canvasCtx.save();
      canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
      
      // 映像を描画 (左右反転はCSSのtransformで行うため、ここでは行わない)
      canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);
      
      let emotion = "Neutral";
      const landmarks = results.multiFaceLandmarks[0];
      
      // フェイスメッシュの描画
      drawConnectors(canvasCtx, landmarks, FACEMESH_TESSELATION, {color: '#80DEEA', lineWidth: 1}); /* 水色系のメッシュ */
      drawConnectors(canvasCtx, landmarks, FACEMESH_RIGHT_EYE,
