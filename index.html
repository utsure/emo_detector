<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MediaPipe Face Mesh</title>
  <style>
    body { font-family: sans-serif; display: flex; justify-content: center; align-items: center; height: 100vh; margin: 0; }
    #video-container { position: relative; }
    #output_canvas { position: absolute; top: 0; left: 0; }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js" crossorigin="anonymous"></script>
</head>

<body>
  <div id="video-container">
    <video id="input_video" width="720" height="560" autoplay muted></video>
    <canvas id="output_canvas" width="720" height="560"></canvas>
  </div>

  <script type="module">
    const videoElement = document.getElementById('input_video');
    const canvasElement = document.getElementById('output_canvas');
    const canvasCtx = canvasElement.getContext('2d');

    
    // onResults関数をまるごとこれに置き換えてください
function onResults(results) {
  // 元々の描画処理
  canvasCtx.save();
  canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
  canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);
  
  let emotion = "Neutral"; // デフォルトの感情を「無表情」に設定

  if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
    const landmarks = results.multiFaceLandmarks[0];

    // 元々のフェイスメッシュ描画処理
    drawConnectors(canvasCtx, landmarks, FACEMESH_TESSELATION, {color: '#C0C0C070', lineWidth: 1});

    // --- ここから新しい感情判定ロジック ---

    // 安定した基準値として、両目の間の距離を計算
    const leftEye = landmarks[263];
    const rightEye = landmarks[33];
    const interEyeDistance = Math.hypot(leftEye.x - rightEye.x, leftEye.y - rightEye.y);

    // 口の主要な点の座標を取得
    const leftMouth = landmarks[61];  // 左口角
    const rightMouth = landmarks[291]; // 右口角
    const topLip = landmarks[13];     // 上唇の中心
    const bottomLip = landmarks[14];  // 下唇の中心

    // 口の横幅と縦幅を計算
    const mouthWidth = Math.hypot(leftMouth.x - rightMouth.x, leftMouth.y - rightMouth.y);
    const mouthHeight = Math.hypot(topLip.x - bottomLip.y, topLip.y - bottomLip.y);
    
    // 各パーツの大きさを基準値で割って、顔の大きさに影響されないようにする
    const normalizedMouthHeight = mouthHeight / interEyeDistance;
    const normalizedMouthWidth = mouthWidth / interEyeDistance;
    
    // 感情を判定する
    if (normalizedMouthHeight > 0.4) {
      emotion = "Surprised"; // 口が大きく開いていたら「驚き」
    } else {
      // 口角が上唇より上にあるかチェック
      const mouthCornerY = (leftMouth.y + rightMouth.y) / 2;
      const upperLipY = topLip.y;
      
      if (mouthCornerY < upperLipY && normalizedMouthWidth > 0.7) {
        emotion = "Happy"; // 口角が上がっていて、口が横に広ければ「笑顔」
      }
    }
    // --- 感情判定ロジックはここまで ---
  }

  // 判定結果を描画
  canvasCtx.font = '50px Arial';
  canvasCtx.fillStyle = 'red';
  canvasCtx.fillText(emotion, 20, 70);

  canvasCtx.restore();
}
    const faceMesh = new FaceMesh({locateFile: (file) => {
      // AIモデルはここから自動で読み込まれる
      return `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`;
    }});
    faceMesh.setOptions({
      maxNumFaces: 1,
      refineLandmarks: true,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5
    });
    faceMesh.onResults(onResults);

    const camera = new Camera(videoElement, {
      onFrame: async () => {
        await faceMesh.send({image: videoElement});
      },
      width: 720,
      height: 560
    });
    camera.start();
  </script>
</body>
</html>
